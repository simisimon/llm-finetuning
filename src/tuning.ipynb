{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup development environment**\n",
    "\n",
    "The first step is to install Hugging Face libraies and PyTorch, including:\n",
    "\n",
    "- torch\n",
    "- trl (transformer reinforcement learning)\n",
    "- transformers\n",
    "- peft (parameter-efficient fine-tuning)\n",
    "- datasets\n",
    "- accelerate\n",
    "- bistandbytes\n",
    "- flash-attn\n",
    "\n",
    "All required libraries are already installed. Just enable the conda environment called tuning (i.e., `conda activate tuning`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "from typing import Optional, List, Dict\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from transformers.utils import is_accelerate_available, is_bitsandbytes_available\n",
    "from transformers.trainer import TRAINER_STATE_NAME\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is accelerate available? \", is_accelerate_available())\n",
    "print(\"Is bitsandbytes available? \", is_bitsandbytes_available())\n",
    "!transformers-cli env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and prepare dataset**\n",
    "\n",
    "There are several ways to create datasets for LLM fine-tuning, including:\n",
    "- using existing open-source datasets\n",
    "- using LLMs to create synthetically datasets\n",
    "- using humans to create datasets \n",
    "- using a combination of the above methods\n",
    "\n",
    "We use an aleardy exsiting dataset from Hugginface called: [CodeInstructions](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing dataset from hugginface with load_dataset()\n",
    "dataset = #TODO\n",
    "\n",
    "# shuffle dataset and select a sample set (e.g., 100 samples)\n",
    "dataset = #TODO\n",
    "\n",
    "# print dataset \n",
    "#TODO\n",
    "\n",
    "# print the instruction, input, and output of a sample from the dataset \n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format dataset**\n",
    "\n",
    "In order to leverage instructing fine-tuning for Mistral, we need to surround our prompts by [INST] and [/INST] tokens. Additionally, the very first instruction should begin with a begin of sequence token and the assistant generation will be ended by the end of sequence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_TEMPLATE = \"\"\"<s>[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
    "\n",
    "### Instruction: {} \n",
    "\n",
    "### Input: {} [/INST] \n",
    "\n",
    "{}</s>\"\"\"\n",
    "\n",
    "# for each sample return a dict with the key 'text', 'instruction', 'input', 'output', where text is the final prompt\n",
    "def format_data(sample: Dict):\n",
    "    # TODO\n",
    "\n",
    "# convert dataset using the format_data() method\n",
    "dataset = # TODO\n",
    "\n",
    "# split dataset into 0.8/0.2 training samples and test sample\n",
    "dataset = # TODO\n",
    "\n",
    "# save train and test datasets to disk\n",
    "# TODO\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model and Tokenizer**\n",
    "\n",
    "1. Define quantization configuration\n",
    "2. Load base model \n",
    "3. Load tokenizer\n",
    "4. Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bnb config with load in 4bits, double quantization, NF4 as quantization type, and torch.float16 as computation type\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# load base model with QLoRA configuration, device_map, and flash-attention\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.unk_token          # we want the pad_token to be different from the eos token\n",
    "tokenizer.padding_side = 'right'                   # prevent warnings\n",
    "\n",
    "# add special tokens to indicate the start and end of a prompt\n",
    "#tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
    "#tokenizer.add_special_tokens({\"eos_token\": \"</s>\"})\n",
    "#tokenizer.add_special_tokens({\"unk_token\": \"<unk>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Print hello world in python c and c++.\"\"\"\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "    output = base_model.generate(input_ids=input_ids, max_new_tokens=1000, pad_token_id=tokenizer.unk_token_id)\n",
    "    response = tokenizer.batch_decode(output.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
    "\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define LoRa Config**\n",
    "\n",
    "Since we do not train all the parameters but only a small subset, we have to add the LoRA to the model using `peft`.\n",
    "\n",
    "1. Find trainable layers\n",
    "2. Define LoRA Config\n",
    "3. Get PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all linear layers\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "modules = find_all_linear_names(base_model)\n",
    "\n",
    "# [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"]\n",
    "\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LoRA config with r, lora_alpha, lora_dropout, target_modules, bias, and task_type\n",
    "lora_config = LoraConfig(\n",
    "    #TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get PEFT model\n",
    "peft_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "trainable, total = peft_model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tune LLM**\n",
    "\n",
    "To fine-tune a LLM, we first need to specify the hyperparameters. Hyperparameters can significantly impact the model performance.\n",
    "\n",
    "1. Define training argumnets\n",
    "2. Define SFTTrainer\n",
    "3. Start Fine-Tuning\n",
    "4. Print Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../data/saved_models/mistral-instruct\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,                                # directory to save and repository id\n",
    "    max_steps=20,                                         # total number of training steps to perform\n",
    "    num_train_epochs=1,                                   # number of training epochs\n",
    "    per_device_train_batch_size=3,                        # batch size per device during training\n",
    "    gradient_accumulation_steps=2,                        # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,                          # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",                            # use fused adamw optimizer\n",
    "    logging_steps=10,                                     # log every 10 steps\n",
    "    save_strategy=\"steps\",                                # save checkpoint every 10th step\n",
    "    save_steps=10,                                        # Number of updates steps before two checkpoint saves\n",
    "    save_total_limit=10,                                  # limit the total amount of checkpoints\n",
    "    learning_rate=2e-4,                                   # learning rate, based on QLoRA paper\n",
    "    bf16=True,                                            # use bfloat16 precision\n",
    "    tf32=True,                                            # use tf32 precision\n",
    "    max_grad_norm=0.3,                                    # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                                    # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",                         # use constant learning rate scheduler\n",
    "    run_name=\"mistral-instruct_{time}\".format(time=time),\n",
    "    report_to=\"mlflow\",                                   # report metrics to mlflow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "train_dataset = # TODO\n",
    "\n",
    "max_seq_length = 8128 # max sequence length for model and packing of the dataset\n",
    "\n",
    "# define SFTTrainer with model, training arguments, train data, peft_config, max_seq_legth, tokeniker, dataset_text_field, packing\n",
    "trainer = SFTTrainer(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "# start training\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "print(metrics) \n",
    "\n",
    "# save model and tokenizer\n",
    "trainer.model.save_pretrained(os.path.join(output_dir, \"final_checkpoint/\"))\n",
    "trainer.tokenizer.save_pretrained(os.path.join(output_dir, \"final_checkpoint/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(scalars: List[float]) -> List[float]:\n",
    "    last = scalars[0]\n",
    "    smoothed = list()\n",
    "    weight = 1.8 * (1 / (1 + math.exp(-0.05 * len(scalars))) - 0.5)  # a sigmoid function\n",
    "    for next_val in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * next_val\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return smoothed\n",
    "\n",
    "\n",
    "def plot_loss(save_directory: os.PathLike, keys: Optional[List[str]] = [\"loss\"]): \n",
    "    with open(os.path.join(save_directory, TRAINER_STATE_NAME), \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for key in keys:\n",
    "        steps, metrics = [], []\n",
    "        for i in range(len(data[\"log_history\"])):\n",
    "            if key in data[\"log_history\"][i]:\n",
    "                steps.append(data[\"log_history\"][i][\"step\"])\n",
    "                metrics.append(data[\"log_history\"][i][key])\n",
    "\n",
    "        if len(metrics) == 0:\n",
    "            print(f\"No metric {key} to plot.\")\n",
    "            continue\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(steps, metrics, alpha=0.4, label=\"original\")\n",
    "        plt.plot(steps, smooth(metrics), label=\"smoothed\")\n",
    "        plt.title(\"training {} of {}\".format(key, save_directory))\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(key)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_directory, \"training_{}.png\".format(key)), format=\"png\", dpi=100)\n",
    "        print(\"Figure saved:\", os.path.join(save_directory, \"training_{}.png\".format(key)))\n",
    "\n",
    "# plot loss\n",
    "plot_loss(save_directory=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference with the fine-tuned Model**\n",
    "\n",
    "1. Load test dataset and prepare test prompt\n",
    "2. Load base model and tokenizer\n",
    "2. Merge weights\n",
    "3. Run inference with fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_dataset = #TODO\n",
    "\n",
    "MISTRAL_INFERENCE_TEMPLATE = \"\"\"<s>[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
    "\n",
    "### Instruction: {} \n",
    "\n",
    "### Input: {} [/INST]\"\"\"\n",
    "\n",
    "# select a test sample\n",
    "test_sample = #TODO\n",
    "\n",
    "# create test prompt\n",
    "prompt = #TODO\n",
    "\n",
    "# print prompt\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    #TODO\n",
    ")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = #TODO\n",
    "tokenizer.pad_token = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify adapter path\n",
    "adapter_path = #TODO\n",
    "\n",
    "# merge LoRA weights with base model base model and save\n",
    "merged_model= PeftModel.from_pretrained(base_model, adapter_path)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# save merged model\n",
    "merged_model.save_pretrained(os.path.join(output_dir, \"final-merged-checkpoint\"), safe_serialization=True, max_shard_size='4GB')\n",
    "tokenizer.save_pretrained(os.path.join(output_dir, \"final-merged-checkpoint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with the fine-tuned model\n",
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
