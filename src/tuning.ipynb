{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup development environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "import mlflow\n",
    "import traceback\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "\n",
    "from random import randrange\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from transformers.utils import is_accelerate_available, is_bitsandbytes_available\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is accelerate available?  True\n",
      "Is bitsandbytes available?  True\n",
      "\n",
      "Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n",
      "\n",
      "- `transformers` version: 4.38.2\n",
      "- Platform: Linux-5.15.0-97-generic-x86_64-with-glibc2.35\n",
      "- Python version: 3.11.7\n",
      "- Huggingface_hub version: 0.20.3\n",
      "- Safetensors version: 0.4.2\n",
      "- Accelerate version: 0.26.1\n",
      "- Accelerate config: \t- compute_environment: LOCAL_MACHINE\n",
      "\t- distributed_type: NO\n",
      "\t- mixed_precision: fp16\n",
      "\t- use_cpu: False\n",
      "\t- debug: False\n",
      "\t- num_processes: 1\n",
      "\t- machine_rank: 0\n",
      "\t- num_machines: 1\n",
      "\t- gpu_ids: all\n",
      "\t- rdzv_backend: static\n",
      "\t- same_network: True\n",
      "\t- main_training_function: main\n",
      "\t- downcast_bf16: no\n",
      "\t- tpu_use_cluster: False\n",
      "\t- tpu_use_sudo: False\n",
      "\t- tpu_env: []\n",
      "- PyTorch version (GPU?): 2.1.2+cu121 (True)\n",
      "- Tensorflow version (GPU?): not installed (NA)\n",
      "- Flax version (CPU?/GPU?/TPU?): not installed (NA)\n",
      "- Jax version: not installed\n",
      "- JaxLib version: not installed\n",
      "- Using GPU in script?: <fill in>\n",
      "- Using distributed or parallel set-up in script?: <fill in>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Is accelerate available? \", is_accelerate_available())\n",
    "print(\"Is bitsandbytes available? \", is_bitsandbytes_available())\n",
    "!transformers-cli env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 25 14:54:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:00:10.0 Off |                    0 |\n",
      "| N/A   28C    P0              35W / 250W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create and prepare Dataset**\n",
    "\n",
    "There are several ways to create datasets for LLM fine-tuning, including:\n",
    "- using existing open-source datasets\n",
    "- using LLMs to create synthetically datasets\n",
    "- using humans to create datasets \n",
    "- using a combination of the above methods\n",
    "\n",
    "We use an aleardy exsiting dataset from Hugginface called: [CodeInstructions](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['output', 'instruction', 'input', 'text'],\n",
      "    num_rows: 100\n",
      "})\n",
      "Instruction:  Generate a code to sort a list of strings alphabetically.\n",
      "input:  \n",
      "Output:  def sortStrings(string_list):\n",
      "    return sorted(string_list)\n"
     ]
    }
   ],
   "source": [
    "# load existing dataset from hugginface\n",
    "dataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(100))\n",
    "\n",
    "print(dataset)\n",
    "print(\"Instruction: \", dataset[0][\"instruction\"])\n",
    "print(\"input: \",dataset[0][\"input\"])\n",
    "print(\"Output: \", dataset[0][\"output\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "MISTRAL_TEMPLATE = \"\"\"<s>[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
    "\n",
    "### Instruction: {} \n",
    "\n",
    "### Input: {} [/INST] \n",
    "\n",
    "{}</s>\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt(sample: Dict):\n",
    "    return {\n",
    "        \"text\": MISTRAL_TEMPLATE.format(sample[\"instruction\"], sample[\"input\"], sample[\"output\"]),\n",
    "        \"instruction\": sample[\"instruction\"],\n",
    "        \"input\": sample[\"input\"],\n",
    "        \"output\": sample[\"output\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2d73255d7e4f8b92748182d170498e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
      "\n",
      "### Instruction: Create a data structure for storing book titles and authors. \n",
      "\n",
      "### Input:  [/INST] \n",
      "\n",
      "HashMap<String, String> books = new HashMap<String, String>(); \n",
      "\n",
      "// Add book titles and authors\n",
      "books.put(\"Harry Potter\", \"JK Rowling\");\n",
      "books.put(\"The Lord of the Rings\", \"JRR Tolkien\");\n",
      "books.put(\"The Alchemist\", \"Paulo Coelho\");\n",
      "books.put(\"The Catcher in the Rye\", \"JD Salinger\");</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4e41ef3a824211b0e488d4bac69934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1365f6f23645d1bcecaa474363b8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "28904"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert dataset\n",
    "dataset = dataset.map(create_prompt, remove_columns=dataset.features)\n",
    "\n",
    "# split dataset into 80 training samples and 20 test sample\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "print(dataset[\"train\"][60][\"text\"])\n",
    "\n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"../data/train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"../data/test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Training Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8550a31b165649f7a5798f13da34b1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load training dataset\n",
    "train_dataset = load_dataset(\"json\", data_files=\"../data/train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model and Tokenizer**\n",
    "\n",
    "1. Define quantization configuration\n",
    "2. Load model \n",
    "3. Load tokenizer\n",
    "4. Inference with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ssimon/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/config.json\n",
      "Model config MistralConfig {\n",
      "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/ssimon/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5a893b60a74863831f93696d20094a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ssimon/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "loading file tokenizer.model from cache at /home/ssimon/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.model\n",
      "loading file tokenizer.json from cache at /home/ssimon/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/ssimon/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/ssimon/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.2/snapshots/41b61a33a2483885c981aa79e0df6b32407ed873/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load bnb config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                             # load in 4bits\n",
    "    bnb_4bit_use_double_quant=True,                # double quantize (quantize weightes and quantize the first quantization constants)\n",
    "    bnb_4bit_quant_type=\"nf4\",                     # use NF4 (normalized fp4)\n",
    "    bnb_4bit_compute_dtype=torch.float16           # compute type is float16 (computations runs in float16)\n",
    ")\n",
    "\n",
    "# load model\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,          # model name\n",
    "    quantization_config=bnb_config,                # bnb config\n",
    "    device_map=\"auto\",                             # auto selects device to put model on; ensures that model is moved to the GPU\n",
    "    attn_implementation=\"flash_attention_2\"        # attention implementation to use in the model\n",
    ")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.unk_token          # we want the pad_token to be different from the eos token\n",
    "\n",
    "# add special tokens to indicate the start and end of a prompt\n",
    "#tokenizer.add_special_tokens({\"bos_token\": \"<s>\"})\n",
    "#tokenizer.add_special_tokens({\"eos_token\": \"</s>\"})\n",
    "#tokenizer.add_special_tokens({\"unk_token\": \"<unk>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
      "\n",
      "### Instruction: Print hello world in python c and c++.[/INST]\n",
      "ikt\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request. \n",
    "\n",
    "### Instruction: Print hello world in python c and c++.[/INST]\"\"\"\n",
    "\n",
    "model_input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=1000, pad_token_id=tokenizer.unk_token_id)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define LoRa Config**\n",
    "\n",
    "1. Find trainable layers\n",
    "2. Define LoRA Config\n",
    "3. Get PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v_proj', 'q_proj', 'gate_proj', 'o_proj', 'k_proj', 'up_proj', 'down_proj']\n"
     ]
    }
   ],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "modules = find_all_linear_names(base_model)\n",
    "\n",
    "# [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"]\n",
    "\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=256,                                       # rank of the update matrices, expressed in `int`. Lower rank results in smaller update matrices with fewer trainable parameters\n",
    "    lora_alpha=128,                              # alpha parameter for LoRA scaling\n",
    "    lora_dropout=0.05,                           # dropout probability for LoRA layers\n",
    "    target_modules=\"all-linear\",                 # name of modules to apply the adapter to\n",
    "    bias=\"none\",                                 # bias type for LoRA\n",
    "    task_type=\"CAUSAL_LM\",                       # type of task to perform\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 671088640 | total: 7912820736 | Percentage: 8.4810%\n"
     ]
    }
   ],
   "source": [
    "# get PEFT model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define training arguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Supervises Finetuning Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start Fine-Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
